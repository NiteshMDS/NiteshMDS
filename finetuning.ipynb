{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67975c-388a-4479-beea-02dbcf127d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "mol = Chem.MolFromSmiles('CO[C@@H]1CC[C@@]2(CC1)Cc1c([C@]32COC(=[NH+]3)N)cc(cc1)c1cncc(c1)C')\n",
    "mol\n",
    "\n",
    "\"\"\"# Import required packages\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.autograd import Function\n",
    "from mxnet.gluon.data import Dataset\n",
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from mxnet.gluon.data.sampler import Sampler\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 6  # training batch size\n",
    "batch_size_test = 6  # test batch size\n",
    "k = 7   # number of generation paths\n",
    "p = 0.8   # randomness parameter alpha\n",
    "F_e = 16    # initial hidden embedding size for each node in a graph\n",
    "F_h = [32, 64, 128, 128, 256, 256]    # output sizes of each GCN layer\n",
    "F_skip = 256    # size of skip connection layer\n",
    "F_c = [512, ]   # hidden sizes of fully connected layers after graph convolution\n",
    "Fh_policy = 128   # hidden size for policy layer\n",
    "activation = 'relu'   # activation function\n",
    "max_epochs = 100    # maximum number of epochs\n",
    "patience = 10   # how many steps in past to check test loss for early stopping\n",
    "lr = 1e-4   # initial learning rate\n",
    "decay = 0.0020    # initial weight decay\n",
    "decay_step = 100    # perform weight decay after 100 steps\n",
    "clip_grad = 3.0    # gradient clipping factor\n",
    "summary_step = 200    # store model and training metrics after every 200 steps\n",
    "N_rnn = 3   # number of layers used in GRUs\n",
    "N_C = 1024   # size of pretrained protein embedding (CDGCN) or one hot encoding (Li et al.))\n",
    "is_continuous = True   # load previous model or not\n",
    "train_only = False  # train only mode or not, in train only mode validation is not done\n",
    "\n",
    "model_name = 'base_cdgcn'\n",
    "model_dir = f'/workspace/Toxicity_experiment/March_experiment/{model_name}'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "ckpt_dir = f'/workspace/Toxicity_experiment/March_experiment/{model_name}/finetune/logs/'    # logs directory\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "pretrained_params_path = f'/workspace/Toxicity_experiment/March_experiment/base_cdgcn-Copy2/logs/ckpt.params'\n",
    "\n",
    "\"\"\"# Read Data\"\"\"\n",
    "\n",
    "def read_data(file_name):\n",
    "    smiles = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            smiles.append(line.strip())\n",
    "    return smiles\n",
    "\n",
    "# Train and test data path\n",
    "t2t_data_train = '/workspace/binding_data/bindingdb/train_dataset/train_4_org_1042_104/d1_tr_cdgcn.txt'\n",
    "dataset_train = read_data(t2t_data_train)\n",
    "if train_only == False:\n",
    "    t2t_data_test = '/workspace/Toxicity_experiment/March_experiment/dataset/d1_te_cdgcn.txt'\n",
    "    dataset_test = read_data(t2t_data_test)\n",
    "\n",
    "print(dataset_train[:2])\n",
    "\n",
    "print(len(dataset_train))\n",
    "print(type(dataset_train))\n",
    "print(dataset_train[:5])\n",
    "if train_only == False:\n",
    "    print(\"train only is false\")\n",
    "    print(len(dataset_test))\n",
    "    print(type(dataset_test))\n",
    "    print(dataset_test[:5])\n",
    "\n",
    "print(dataset_train[6])\n",
    "\n",
    "len(dataset_train[1])\n",
    "\n",
    "\"\"\"# Creating dataset iterable\"\"\"\n",
    "\n",
    "class IterableDataset(Dataset):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self[i] for i in range(len(self)))\n",
    "\n",
    "\n",
    "class Lambda(IterableDataset):\n",
    "    \"\"\"\n",
    "    Preprocessing fn\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, fn=lambda _x: _x):\n",
    "        random.seed(17)\n",
    "        random.shuffle(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.fn = fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.fn(self.dataset[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\"\"\"# Creating mini-batches\"\"\"\n",
    "\n",
    "class BalancedSampler(Sampler):\n",
    "\n",
    "    def __init__(self, cost, batch_size):\n",
    "        index = np.argsort(cost).tolist()\n",
    "        chunk_size = int(float(len(cost))/batch_size)\n",
    "        self.index = []\n",
    "        for i in range(batch_size):\n",
    "            self.index.append(index[i*chunk_size:(i + 1)*chunk_size])\n",
    "\n",
    "    def _g(self):\n",
    "        # shuffle data\n",
    "        for index_i in self.index:\n",
    "            random.shuffle(index_i)\n",
    "\n",
    "        for batch_index in zip(*self.index):\n",
    "            yield batch_index\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._g()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index[0])\n",
    "\n",
    "\"\"\"\n",
    "# Separating smiles and protein embedding\"\"\"\n",
    "\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Suppress RDKit warnings and errors\n",
    "#RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "class Conditional(object):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Delimited(Conditional):\n",
    "\n",
    "    def __init__(self, d='\\t'):\n",
    "        self.d = d\n",
    "\n",
    "    def __call__(self, line):\n",
    "        line = line.strip('\\n').strip('\\r')\n",
    "        line = line.split(self.d)\n",
    "\n",
    "        smiles = line[0]\n",
    "        c = np.array([float(c_i) for c_i in line[1:]], dtype=np.float32)\n",
    "\n",
    "        return smiles, c\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# Obtain all molecular properties\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# Utility functions for data preprocessing\"\"\"\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, autograd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "\n",
    "\"\"\"# Obtaining all molecular properties\"\"\"\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Load the classifier\n",
    "loaded_model = joblib.load('/home/niteshs/mtp/workspace/Toxicity_experiment/tox_ml_classifiers/random_forest_model.joblib')\n",
    "\n",
    "def smiles_to_morgan_fingerprints(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def predict_single_smiles(smiles):\n",
    "    fingerprint = smiles_to_morgan_fingerprints(smiles)\n",
    "    if fingerprint is not None:\n",
    "        prediction_prob = loaded_model.predict_proba([fingerprint])\n",
    "        prediction_prob = prediction_prob[0]\n",
    "        prediction = loaded_model.predict([fingerprint])\n",
    "        #print(prediction_prob)\n",
    "        #print(prediction)\n",
    "        return prediction_prob[1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example of how to use the function to predict on a single SMILES\n",
    "single_smiles = \"CCO\"\n",
    "prediction = predict_single_smiles(single_smiles)\n",
    "\n",
    "if prediction is not None:\n",
    "    print(f'Prediction for SMILES \"{single_smiles}\": {prediction}')\n",
    "else:\n",
    "    print(f'Could not process SMILES \"{single_smiles}\"')\n",
    "\n",
    "\n",
    "class MoleculeSpec(object):\n",
    "\n",
    "    def __init__(self, file_name='/workspace/binding_data/atom_types.txt'):\n",
    "        self.atom_types = []\n",
    "        self.atom_symbols = []\n",
    "        with open(file_name) as f:\n",
    "            for line in f:\n",
    "                atom_type_i = line.strip('\\n').split(',')\n",
    "                self.atom_types.append((atom_type_i[0], int(atom_type_i[1]), int(atom_type_i[2])))\n",
    "                if atom_type_i[0] not in self.atom_symbols:\n",
    "                    self.atom_symbols.append(atom_type_i[0])\n",
    "        self.bond_orders = [Chem.BondType.AROMATIC,\n",
    "                            Chem.BondType.SINGLE,\n",
    "                            Chem.BondType.DOUBLE,\n",
    "                            Chem.BondType.TRIPLE]\n",
    "        self.max_iter = 120\n",
    "\n",
    "    def get_atom_type(self, atom):\n",
    "        atom_symbol = atom.GetSymbol()\n",
    "        atom_charge = atom.GetFormalCharge()\n",
    "        atom_hs = atom.GetNumExplicitHs()\n",
    "        return self.atom_types.index((atom_symbol, atom_charge, atom_hs))\n",
    "\n",
    "    def get_bond_type(self, bond):\n",
    "        return self.bond_orders.index(bond.GetBondType())\n",
    "\n",
    "    def index_to_atom(self, idx):\n",
    "        atom_symbol, atom_charge, atom_hs = self.atom_types[idx]\n",
    "        a = Chem.Atom(atom_symbol)\n",
    "        a.SetFormalCharge(atom_charge)\n",
    "        a.SetNumExplicitHs(atom_hs)\n",
    "        return a\n",
    "\n",
    "    def index_to_bond(self, mol, begin_id, end_id, idx):\n",
    "        mol.AddBond(begin_id, end_id, self.bond_orders[idx])\n",
    "\n",
    "    @property\n",
    "    def num_atom_types(self):\n",
    "        return len(self.atom_types)\n",
    "\n",
    "    @property\n",
    "    def num_bond_types(self):\n",
    "        return len(self.bond_orders)\n",
    "\n",
    "_mol_spec = None\n",
    "\n",
    "def get_mol_spec():\n",
    "    global _mol_spec\n",
    "    if _mol_spec is None:\n",
    "        _mol_spec = MoleculeSpec()\n",
    "    return _mol_spec\n",
    "\n",
    "def get_graph_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # build graph\n",
    "    atom_types, atom_ranks, bonds, bond_types = [], [], [], []\n",
    "    for a, r in zip(mol.GetAtoms(), Chem.CanonicalRankAtoms(mol)):\n",
    "        atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        atom_ranks.append(r)\n",
    "    for b in mol.GetBonds():\n",
    "        idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "        bonds.append([idx_1, idx_2])\n",
    "        bond_types.append(bt)\n",
    "\n",
    "    # build nx graph\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(range(len(atom_types)))\n",
    "    graph.add_edges_from(bonds)\n",
    "\n",
    "    return graph, atom_types, atom_ranks, bonds, bond_types\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_graph_from_smiles_list(smiles_list):\n",
    "    graph_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        # build graph\n",
    "        atom_types, bonds, bond_types = [], [], []\n",
    "        for a in mol.GetAtoms():\n",
    "            atom_types.append(get_mol_spec().get_atom_type(a))\n",
    "        for b in mol.GetBonds():\n",
    "            idx_1, idx_2, bt = b.GetBeginAtomIdx(), b.GetEndAtomIdx(), get_mol_spec().get_bond_type(b)\n",
    "            bonds.append([idx_1, idx_2])\n",
    "            bond_types.append(bt)\n",
    "\n",
    "        X_0 = np.array(atom_types, dtype=np.int32)\n",
    "        A_0 = np.concatenate([np.array(bonds, dtype=np.int32),\n",
    "                              np.array(bond_types, dtype=np.int32)[:, np.newaxis]],\n",
    "                             axis=1)\n",
    "        graph_list.append([X_0, A_0])\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "\n",
    "def traverse_graph(graph, atom_ranks, current_node=None, step_ids=None, p=0.9, log_p=0.0):\n",
    "    if current_node is None:\n",
    "        next_nodes = range(len(atom_ranks))\n",
    "        step_ids = [-1, ] * len(next_nodes)\n",
    "        next_node_ranks = atom_ranks\n",
    "    else:\n",
    "        next_nodes = graph.neighbors(current_node)  # get neighbor nodes\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "        next_node_ranks = [atom_ranks[n] for n in next_nodes] # get ranks for neighbors\n",
    "    next_nodes = [n for n, r in sorted(zip(next_nodes, next_node_ranks), key=lambda _x:_x[1])] # sort by rank\n",
    "\n",
    "    # iterate through neighbors\n",
    "    while len(next_nodes) > 0:\n",
    "        if len(next_nodes)==1:\n",
    "            next_node = next_nodes[0]\n",
    "        elif random.random() >= (1 - p):\n",
    "            next_node = next_nodes[0]\n",
    "            log_p += np.log(p)\n",
    "        else:\n",
    "            next_node = next_nodes[random.randint(1, len(next_nodes) - 1)]\n",
    "            log_p += np.log((1.0 - p) / (len(next_nodes) - 1))\n",
    "        step_ids[next_node] = max(step_ids) + 1\n",
    "        _, log_p = traverse_graph(graph, atom_ranks, next_node, step_ids, p, log_p)\n",
    "        next_nodes = [n for n in next_nodes if step_ids[n] < 0] # filter visited nodes\n",
    "\n",
    "    return step_ids, log_p\n",
    "\n",
    "\n",
    "def single_reorder(X_0, A_0, step_ids):\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    step_ids = np.array(step_ids, dtype=np.int32)\n",
    "\n",
    "    # sort by step_ids\n",
    "    sorted_ids = np.argsort(step_ids)\n",
    "    X_0 = X_0[sorted_ids]\n",
    "    A_0[:, 0], A_0[:, 1] = step_ids[A_0[:, 0]], step_ids[A_0[:, 1]]\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    A_0 = A_0[np.lexsort([-min_b, max_b]), :]\n",
    "\n",
    "    # separate append and connect\n",
    "    max_b, min_b = np.amax(A_0[:, :2], axis=1), np.amin(A_0[:, :2], axis=1)\n",
    "    is_append = np.concatenate([np.array([True]), max_b[1:] > max_b[:-1]])\n",
    "    A_0 = np.concatenate([np.where(is_append[:, np.newaxis],\n",
    "                                 np.stack([min_b, max_b], axis=1),\n",
    "                                 np.stack([max_b, min_b], axis=1)),\n",
    "                        A_0[:, -1:]], axis=1)\n",
    "    return X_0, A_0\n",
    "\n",
    "\n",
    "def single_expand(X_0, A_0):\n",
    "    X_0, A_0 = np.copy(X_0), np.copy(A_0)\n",
    "\n",
    "    # expand X\n",
    "    is_append_iter = np.less(A_0[:, 0], A_0[:, 1]).astype(np.int32)\n",
    "    NX = np.cumsum(np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1))\n",
    "    shift = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    X_index = np.arange(NX.sum(), dtype=np.int32) - np.repeat(shift, NX)\n",
    "\n",
    "\n",
    "    X = X_0[X_index]\n",
    "    # expand A\n",
    "    _, A_index = np.tril_indices(A_0.shape[0])\n",
    "    A = A_0[A_index, :]\n",
    "    # Almost same thing being done to A\n",
    "    NA = np.arange(A_0.shape[0] + 1)                  # Num_of_bonds + 1\n",
    "\n",
    "    # get action\n",
    "    # action_type, atom_type, bond_type, append_pos, connect_pos\n",
    "    action_type = 1 - is_append_iter\n",
    "    atom_type = np.where(action_type == 0, X_0[A_0[:, 1]], 0)\n",
    "    bond_type = A_0[:, 2]\n",
    "    append_pos = np.where(action_type == 0, A_0[:, 0], 0)\n",
    "    connect_pos = np.where(action_type == 1, A_0[:, 1], 0)\n",
    "    actions = np.stack([action_type, atom_type, bond_type, append_pos, connect_pos],\n",
    "                       axis=1)\n",
    "    last_action = [[2, 0, 0, 0, 0]]\n",
    "    actions = np.append(actions, last_action, axis=0)\n",
    "    action_0 = np.array([X_0[0]], dtype=np.int32)\n",
    "\n",
    "\n",
    "    # Get mask\n",
    "    last_atom_index = shift + NX - 1\n",
    "    last_atom_mask = np.zeros_like(X)\n",
    "    last_atom_mask[last_atom_index] = np.where(\n",
    "        np.pad(is_append_iter, [[1, 0]], mode='constant', constant_values=1) == 1,\n",
    "        np.ones_like(last_atom_index),\n",
    "        np.ones_like(last_atom_index) * 2)\n",
    "    \n",
    "\n",
    "    return action_0, X, NX, A, NA, actions, last_atom_mask\n",
    "\n",
    "\n",
    "def get_d(A, X):\n",
    "    _to_sparse = lambda _A, _X: sparse.coo_matrix((np.ones([_A.shape[0] * 2], dtype=np.int32),\n",
    "                                                   (np.concatenate([_A[:, 0], _A[:, 1]], axis=0),\n",
    "                                                    np.concatenate([_A[:, 1], _A[:, 0]], axis=0))),\n",
    "                                                    shape=[_X.shape[0], ] * 2)   # this is creating a square matrtix of size len(X)\n",
    "\n",
    "\n",
    "    A_sparse = _to_sparse(A, X)\n",
    "\n",
    "    d2 = A_sparse * A_sparse\n",
    "    \n",
    "    d3 = d2 * A_sparse\n",
    "\n",
    "    # get D_2\n",
    "    D_2 = np.stack(d2.nonzero(), axis=1)\n",
    "    D_2 = D_2[D_2[:, 0] < D_2[:, 1], :]\n",
    "\n",
    "    # get D_3\n",
    "    D_3 = np.stack(d3.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    # remove D_1 elements from D_3\n",
    "    D_3_sparse = _to_sparse(D_3, X)\n",
    "    D_3_sparse = D_3_sparse - D_3_sparse.multiply(A_sparse)\n",
    "    D_3 = np.stack(D_3_sparse.nonzero(), axis=1)\n",
    "    D_3 = D_3[D_3[:, 0] < D_3[:, 1], :]\n",
    "\n",
    "    return D_2, D_3\n",
    "\n",
    "\n",
    "def merge_single_0(X_0, A_0, NX_0, NA_0):\n",
    "    # shift_ids\n",
    "    cumsum = np.cumsum(np.pad(NX_0, [[1, 0]], mode='constant')[:-1])\n",
    "    A_0[:, :2] += np.stack([np.repeat(cumsum, NA_0), ] * 2, axis=1)\n",
    "\n",
    "    # get D\n",
    "    D_0_2, D_0_3 = get_d(A_0, X_0)\n",
    "\n",
    "    # split A for different bonds\n",
    "    A_split = []\n",
    "    for i in range(get_mol_spec().num_bond_types):\n",
    "        A_i = A_0[A_0[:, 2] == i, :2]\n",
    "        A_split.append(A_i)\n",
    "    A_split.extend([D_0_2, D_0_3])\n",
    "    A_0 = A_split\n",
    "\n",
    "    # NX_rep\n",
    "    NX_rep_0 = np.repeat(np.arange(NX_0.shape[0]), NX_0)\n",
    "\n",
    "    \n",
    "\n",
    "    return X_0, A_0, NX_0, NX_rep_0\n",
    "\n",
    "\n",
    "def merge_single(X, A, NX, NA, mol_ids, rep_ids, iw_ids, action_0, actions, last_append_mask, log_p):\n",
    "    X, A, NX, NX_rep = merge_single_0(X, A, NX, NA)\n",
    "    cumsum = np.cumsum(np.pad(NX, [[1, 0]], mode='constant')[:-1])\n",
    "    actions[:, -2] += cumsum * (actions[:, 0] == 0)\n",
    "    actions[:, -1] += cumsum * (actions[:, 0] == 1)\n",
    "    mol_ids_rep = np.repeat(mol_ids, NX)\n",
    "    rep_ids_rep = np.repeat(rep_ids, NX)\n",
    "\n",
    "    return X, A,\\\n",
    "           mol_ids_rep, rep_ids_rep, iw_ids,\\\n",
    "           last_append_mask,\\\n",
    "           NX, NX_rep,\\\n",
    "           action_0, actions, \\\n",
    "           log_p\n",
    "\n",
    "\"\"\"def predict_score_wrapper(smile):\n",
    "    return predict_score(custom_gcn, smile)\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\"\"\"\n",
    "\n",
    "def process_single(smiles, k, p):\n",
    "    # smiles, smiles_class = smiles.split(\" \")\n",
    "    # smiles_class = int(smiles_class)\n",
    "    graph, atom_types, atom_ranks, bonds, bond_types = get_graph_from_smiles(smiles)\n",
    "    \n",
    "    try:\n",
    "        graph, atom_types, atom_ranks, bonds, bond_types = get_graph_from_smiles(smiles)\n",
    "        X_0 = np.array(atom_types, dtype=np.int32)\n",
    "        A_0 = np.concatenate([np.array(bonds, dtype=np.int32),\n",
    "                                  np.array(bond_types, dtype=np.int32)[:, np.newaxis]], axis=1)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error processing SMILES: {smiles}\")\n",
    "            print(f\"Error message: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    toxic_prob =  predict_single_smiles(smiles)\n",
    "    if toxic_prob > 0.50 :\n",
    "        toxic_class = 1\n",
    "    else:\n",
    "        toxic_class = 0 \n",
    "\n",
    "    X, A = [], []\n",
    "    NX, NA = [], []\n",
    "    mol_ids, rep_ids, iw_ids = [], [], []\n",
    "    action_0, actions = [], []\n",
    "    last_append_mask = []\n",
    "    log_p = []\n",
    "    tox_class = [toxic_class] * k\n",
    "\n",
    "    # random sampling decoding routes\n",
    "    \n",
    "\n",
    "    for i in range(k):\n",
    "        \n",
    "        \n",
    "        step_ids_i, log_p_i = traverse_graph(graph, atom_ranks, p=p)\n",
    "       \n",
    "        X_i, A_i = single_reorder(X_0, A_0, step_ids_i)\n",
    "        action_0_i, X_i, NX_i, A_i, NA_i, actions_i, last_atom_mask_i = single_expand(X_i, A_i)\n",
    "\n",
    "        # appends\n",
    "        X.append(X_i)\n",
    "        A.append(A_i)\n",
    "        NX.append(NX_i)\n",
    "        NA.append(NA_i)\n",
    "        action_0.append(action_0_i)\n",
    "        actions.append(actions_i)\n",
    "        last_append_mask.append(last_atom_mask_i)\n",
    "\n",
    "        mol_ids.append(np.zeros_like(NX_i, dtype=np.int32))\n",
    "        rep_ids.append(np.ones_like(NX_i, dtype=np.int32) * i)\n",
    "        iw_ids.append(np.ones_like(NX_i, dtype=np.int32) * i)\n",
    "\n",
    "        log_p.append(log_p_i)\n",
    "        #tox_class.append(tox_class_i)\n",
    "\n",
    "    # concatenate\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    A = np.concatenate(A, axis = 0)\n",
    "    NX = np.concatenate(NX, axis = 0)\n",
    "    NA = np.concatenate(NA, axis = 0)\n",
    "    action_0 = np.concatenate(action_0, axis = 0)\n",
    "    actions = np.concatenate(actions, axis = 0)\n",
    "    last_append_mask = np.concatenate(last_append_mask, axis = 0)\n",
    "    mol_ids = np.concatenate(mol_ids, axis = 0)\n",
    "    rep_ids = np.concatenate(rep_ids, axis = 0)\n",
    "    iw_ids = np.concatenate(iw_ids, axis = 0)\n",
    "    log_p = np.array(log_p, dtype=np.float32)\n",
    "    tox_class = np.array(tox_class)\n",
    "\n",
    "    return X, A, NX, NA, mol_ids, rep_ids, iw_ids, action_0, actions, last_append_mask, log_p, tox_class\n",
    "\n",
    "# noinspection PyArgumentList\n",
    "def get_mol_from_graph(X, A, sanitize=True):\n",
    "    try:\n",
    "        mol = Chem.RWMol(Chem.Mol())\n",
    "\n",
    "        X, A = X.tolist(), A.tolist()\n",
    "        for i, atom_type in enumerate(X):\n",
    "            mol.AddAtom(get_mol_spec().index_to_atom(atom_type))\n",
    "\n",
    "        for atom_id1, atom_id2, bond_type in A:\n",
    "            get_mol_spec().index_to_bond(mol, atom_id1, atom_id2, bond_type)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    if sanitize:\n",
    "        try:\n",
    "            mol = mol.GetMol()\n",
    "            Chem.SanitizeMol(mol)\n",
    "            return mol\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return mol\n",
    "\n",
    "def get_mol_from_graph_list(graph_list, sanitize=True):\n",
    "    mol_list = [get_mol_from_graph(X, A, sanitize) for X, A in graph_list]\n",
    "    return mol_list\n",
    "\n",
    "\"\"\"# Loading data on GPU\"\"\"\n",
    "\n",
    "class MolLoader(DataLoader):\n",
    "    \"\"\"Load graph based molecule representation from SMILES\"\"\"\n",
    "    def __init__(self, dataset, batch_size=10, num_workers=0,\n",
    "                 k=10, p=0.9, shuffle=False, sampler=None, batch_sampler=None):\n",
    "        self.k = k\n",
    "        self.p = p\n",
    "\n",
    "        # batch_sampler, sampler and shuffle are mutually exclusive\n",
    "        if batch_sampler is not None:\n",
    "            super(MolLoader, self).__init__(dataset, batch_sampler=batch_sampler,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn)\n",
    "        elif sampler is not None:\n",
    "            super(MolLoader, self).__init__(dataset, sampler=sampler,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn,\n",
    "                                            last_batch='rollover')\n",
    "        else:\n",
    "            super(MolLoader, self).__init__(dataset, batch_size, shuffle=shuffle,\n",
    "                                            num_workers=num_workers, batchify_fn=self._collate_fn,\n",
    "                                            last_batch='rollover')\n",
    "\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        # names = X, A,\n",
    "        #         NX, NA,\n",
    "        #         mol_ids, rep_ids, iw_ids,\n",
    "        #         action_0, actions,\n",
    "        #         last_append_mask,\n",
    "        #         log_p\n",
    "\n",
    "        shapes = [[0], [0, 3],\n",
    "                  [0], [0],\n",
    "                  [0], [0], [0],\n",
    "                  [0], [0, 5],\n",
    "                  [0],\n",
    "                  [0], [0]]\n",
    "        dtypes = [np.int32, np.int32,\n",
    "                  np.int32, np.int32,\n",
    "                  np.int32, np.int32, np.int32,\n",
    "                  np.int32, np.int32,\n",
    "                  np.int32,\n",
    "                  np.float32, np.int32]\n",
    "\n",
    "        _build = lambda: [np.zeros(shape=s, dtype=d) for s, d in zip(shapes, dtypes)]\n",
    "        _append = lambda _r0, _r1: [np.concatenate([__r0, __r1], axis=0)\n",
    "                                    for __r0, __r1 in zip(_r0, _r1)]\n",
    "\n",
    "        X, A, \\\n",
    "        NX, NA, \\\n",
    "        mol_ids, rep_ids, iw_ids, \\\n",
    "        action_0, actions, \\\n",
    "        last_append_mask, \\\n",
    "        log_p, tox_class = _build()\n",
    "\n",
    "        # Processing batch of molecules\n",
    "        \n",
    "        for i, record_in in enumerate(batch):\n",
    "            #print(f\"{i}th smiles is {record_in}\")\n",
    "            smiles = record_in\n",
    "\n",
    "            # Using process single\n",
    "            X_i, A_i, \\\n",
    "            NX_i, NA_i, \\\n",
    "            mol_ids_i, rep_ids_i, iw_ids_i, \\\n",
    "            action_0_i, actions_i, \\\n",
    "            last_append_mask_i, log_p_i, tox_class_i = process_single(smiles, self.k, self.p)\n",
    "\n",
    "            if i != 0:\n",
    "                mol_ids_i += mol_ids[-1] + 1\n",
    "                iw_ids_i += iw_ids[-1] + 1\n",
    "\n",
    "            # appending to the list\n",
    "            tox_class_i = tox_class_i.reshape(-1)\n",
    "\n",
    "\n",
    "            X, A, \\\n",
    "            NX, NA, \\\n",
    "            mol_ids, rep_ids, iw_ids, \\\n",
    "            action_0, actions, \\\n",
    "            last_append_mask, \\\n",
    "            log_p, tox_class = _append([X, A,\n",
    "                             NX, NA,\n",
    "                             mol_ids, rep_ids, iw_ids,\n",
    "                             action_0, actions,\n",
    "                             last_append_mask,\n",
    "                             log_p, tox_class],\n",
    "                            [X_i, A_i,\n",
    "                             NX_i, NA_i,\n",
    "                             mol_ids_i, rep_ids_i, iw_ids_i,\n",
    "                             action_0_i, actions_i,\n",
    "                             last_append_mask_i,\n",
    "                             log_p_i, tox_class_i])\n",
    "\n",
    "        \n",
    "        # using merge single\n",
    "        X, A, \\\n",
    "        mol_ids_rep, rep_ids_rep, iw_ids, \\\n",
    "        last_append_mask, \\\n",
    "        NX, NX_rep, \\\n",
    "        action_0, actions, \\\n",
    "        log_p = merge_single(X, A,\n",
    "                                   NX, NA,\n",
    "                                   mol_ids, rep_ids, iw_ids,\n",
    "                                   action_0, actions,\n",
    "                                   last_append_mask,\n",
    "                                   log_p)\n",
    "        \n",
    "\n",
    "        # returning all the data structures combined as list\n",
    "        result_out = [X, A,\n",
    "                      mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                      last_append_mask,\n",
    "                      NX, NX_rep,\n",
    "                      action_0, actions,\n",
    "                      log_p, tox_class]\n",
    "\n",
    "        return result_out\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        \"\"\"Convert numpy to tensor and place it to a specific device\"\"\"\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p, tox_class] = record\n",
    "\n",
    "        # retaining the record output from collate function defined above\n",
    "        \n",
    "        #This is the combined list of atoms_list for molecules in this bartch\n",
    "        X = nd.array(X, ctx=mx.gpu(), dtype='int32')\n",
    "\n",
    "      \n",
    "        A_sparse = []\n",
    "        for A_i in A:\n",
    "            \n",
    "            if A_i.shape[0] == 0:\n",
    "                A_sparse.append(None)\n",
    "                #print(f\"We had to append none to A_sparse\")\n",
    "            else:\n",
    "                # transpose may not be supported in gpu\n",
    "                A_i = np.concatenate([A_i, A_i[:, [1, 0]]], axis=0)\n",
    "\n",
    "                # construct csr matrix ...\n",
    "                data = np.ones((A_i.shape[0], ), dtype=np.float32)\n",
    "                row, col = A_i[:, 0], A_i[:, 1]\n",
    "                A_sparse_i = nd.sparse.csr_matrix((data, (row, col)),\n",
    "                                                  shape=tuple([int(X.shape[0]), ]*2),\n",
    "                                                  ctx=mx.gpu(),\n",
    "                                                  dtype='float32')\n",
    "\n",
    "                # append to list\n",
    "                \n",
    "                A_sparse.append(A_sparse_i)\n",
    "\n",
    "        batch_size, iw_size = (mol_ids_rep.max() + 1).item(), \\\n",
    "                              (rep_ids_rep.max() + 1).item()\n",
    "\n",
    "        mol_ids_rep, rep_ids_rep, iw_ids, \\\n",
    "        last_append_mask, \\\n",
    "        NX, NX_rep, action_0, actions = [nd.array(_x, ctx=mx.gpu(), dtype='int32')\n",
    "                                         for _x in [mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                    last_append_mask,\n",
    "                                                    NX, NX_rep, action_0, actions]]\n",
    "\n",
    "        log_p = nd.array(log_p, ctx=mx.gpu(), dtype='float32')\n",
    "        tox_class = nd.array(tox_class, ctx=mx.gpu())\n",
    "        #print(f\"Length of log_p is {log_p}\")\n",
    "        record = [X, A_sparse, iw_ids, last_append_mask,\n",
    "                  NX, NX_rep, action_0, actions, log_p,\n",
    "                  batch_size, iw_size, tox_class]\n",
    "\n",
    "\n",
    "        return record\n",
    "\n",
    "\n",
    "class MolRNNLoader(MolLoader):\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        result_out = super(MolRNNLoader, self)._collate_fn(batch)\n",
    "\n",
    "        # things ready for rnn\n",
    "        # batch = [smile.split(\" \")[0] for smile in batch]\n",
    "        mol_list = [Chem.MolFromSmiles(batch_i) for batch_i in batch]\n",
    "        # preparing mapping\n",
    "        graph_to_rnn = np.zeros((len(batch), self.k, get_mol_spec().max_iter), dtype=np.int32)\n",
    "        rnn_to_graph = []\n",
    "        cum_sum = 0\n",
    "        for i, mol_i in enumerate(mol_list):\n",
    "            num_iter = mol_i.GetNumBonds() + 1\n",
    "            for k in range(self.k):\n",
    "                graph_to_rnn[i, k, :num_iter] = (np.arange(num_iter) + cum_sum)\n",
    "\n",
    "                rnn_to_graph_0 = np.ones([num_iter,], dtype=np.int32) * i\n",
    "                rnn_to_graph_1 = np.ones_like(rnn_to_graph_0) * k\n",
    "                rnn_to_graph_2 = np.arange(num_iter)\n",
    "                rnn_to_graph.append(np.stack([rnn_to_graph_0, rnn_to_graph_1, rnn_to_graph_2], axis=0))\n",
    "\n",
    "                cum_sum += num_iter\n",
    "        rnn_to_graph = np.concatenate(rnn_to_graph, axis=1)\n",
    "        NX_cum = np.cumsum(result_out[6])\n",
    "\n",
    "        result_out = result_out + [graph_to_rnn, rnn_to_graph, NX_cum]\n",
    "\n",
    "        return result_out\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p, tox_class,\n",
    "         graph_to_rnn, rnn_to_graph, NX_cum] = record\n",
    "\n",
    "        output = MolLoader.from_numpy_to_tensor([X, A,\n",
    "                                                 mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                 last_append_mask,\n",
    "                                                 NX, NX_rep,\n",
    "                                                 action_0, actions,\n",
    "                                                 log_p, tox_class])\n",
    "\n",
    "        graph_to_rnn, rnn_to_graph, NX_cum =\\\n",
    "            nd.array(graph_to_rnn, ctx=mx.gpu(), dtype='int32'),\\\n",
    "            nd.array(rnn_to_graph, ctx=mx.gpu(), dtype='int32'), \\\n",
    "            nd.array(NX_cum, ctx=mx.gpu(), dtype='int32')\n",
    "\n",
    "        output = output + [graph_to_rnn, rnn_to_graph, NX_cum]\n",
    "\n",
    "        return output\n",
    "        return output\n",
    "\n",
    "class CMolRNNLoader(MolRNNLoader):\n",
    "\n",
    "    def __init__(self, dataset, batch_size=10, num_workers=0,\n",
    "                 k=10, p=0.9, shuffle=False, sampler=None, batch_sampler=None,\n",
    "                 conditional=None):\n",
    "        if conditional is None:\n",
    "            raise ValueError('Conditional function is not set, '\n",
    "                             'use unconditional version instead')\n",
    "        if not callable(conditional):\n",
    "            raise TypeError('Provided condition is not callable')\n",
    "\n",
    "        self.conditional = conditional\n",
    "\n",
    "        super(CMolRNNLoader, self).__init__(dataset, batch_size, num_workers,\n",
    "                                            k, p, shuffle, sampler, batch_sampler)\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        smiles_list, c = [], []\n",
    "        for record_i in batch:\n",
    "            smiles_i, c_i = self.conditional(record_i)\n",
    "            smiles_list.append(smiles_i)\n",
    "            c.append(c_i)\n",
    "        c = np.stack(c, axis=0)\n",
    "\n",
    "        output = super(CMolRNNLoader, self)._collate_fn(smiles_list)\n",
    "        output.append(c)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy_to_tensor(record):\n",
    "        [X, A,\n",
    "         mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "         last_append_mask,\n",
    "         NX, NX_rep,\n",
    "         action_0, actions,\n",
    "         log_p, tox_class,\n",
    "         graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "         c] = record\n",
    "\n",
    "        output = MolRNNLoader.from_numpy_to_tensor([X, A,\n",
    "                                                    mol_ids_rep, rep_ids_rep, iw_ids,\n",
    "                                                    last_append_mask,\n",
    "                                                    NX, NX_rep,\n",
    "                                                    action_0, actions,\n",
    "                                                    log_p,tox_class,\n",
    "                                                    graph_to_rnn, rnn_to_graph, NX_cum])\n",
    "        ids = nd.array(mol_ids_rep, ctx=mx.gpu(), dtype='int64')\n",
    "\n",
    "        c = nd.array(c, ctx=mx.gpu(), dtype='float32')\n",
    "\n",
    "        output = output + [c, ids]\n",
    "\n",
    "        return output\n",
    "\n",
    "\"\"\"# Defining graph convolution and other functions\"\"\"\n",
    "\n",
    "class GraphConvFn(Function):\n",
    "\n",
    "    def __init__(self, A):\n",
    "        self.A = A # type: nd.sparse.CSRNDArray\n",
    "        self.A_T = self.A # assume symmetric\n",
    "        super(GraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.A is not None:\n",
    "            if len(X.shape) > 2:\n",
    "                X_resized = X.reshape((X.shape[0], -1))\n",
    "                output = nd.sparse.dot(self.A, X_resized)\n",
    "                output = output.reshape([-1, ] + [X.shape[i] for i in range(1, len(X.shape))])\n",
    "            else:\n",
    "                output = nd.sparse.dot(self.A, X)\n",
    "            return output\n",
    "        else:\n",
    "            return nd.zeros_like(X)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "\n",
    "        if self.A is not None:\n",
    "            if len(grad_output.shape) > 2:\n",
    "                grad_output_resized = grad_output.reshape((grad_output.shape[0], -1))\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output_resized)\n",
    "                grad_input = grad_input.reshape([-1] + [grad_output.shape[i]\n",
    "                                                        for i in range(1, len(grad_output.shape))])\n",
    "            else:\n",
    "                grad_input = nd.sparse.dot(self.A_T, grad_output)\n",
    "            return grad_input\n",
    "        else:\n",
    "            return nd.zeros_like(grad_output)\n",
    "\n",
    "\n",
    "class EfficientGraphConvFn(Function):\n",
    "    \"\"\"Save memory by re-computation\"\"\"\n",
    "\n",
    "    def __init__(self, A_list):\n",
    "        self.A_list = A_list\n",
    "        super(EfficientGraphConvFn, self).__init__()\n",
    "\n",
    "    def forward(self, X, W):\n",
    "        X_list = [X]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "        self.save_for_backward(X, W)\n",
    "\n",
    "        return nd.dot(X_out, W)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        X, W = self.saved_tensors\n",
    "\n",
    "        # recompute X_out\n",
    "        X_list = [X, ]\n",
    "        for A in self.A_list:\n",
    "            if A is not None:\n",
    "                X_list.append(nd.sparse.dot(A, X))\n",
    "            else:\n",
    "                X_list.append(nd.zeros_like(X))\n",
    "        X_out = nd.concat(*X_list, dim=1)\n",
    "\n",
    "        grad_W = nd.dot(X_out.T, grad_output)\n",
    "\n",
    "        grad_X_out = nd.dot(grad_output, W.T)\n",
    "        grad_X_out_list = nd.split(grad_X_out, num_outputs=len(self.A_list) + 1)\n",
    "\n",
    "\n",
    "        grad_X = [grad_X_out_list[0], ]\n",
    "        for A, grad_X_out in zip(self.A_list, grad_X_out_list[1:]):\n",
    "            if A is not None:\n",
    "                grad_X.append(nd.sparse.dot(A, grad_X_out))\n",
    "            else:\n",
    "                grad_X.append(nd.zeros_like(grad_X_out))\n",
    "\n",
    "        grad_X = sum(grad_X)\n",
    "\n",
    "        return grad_X, grad_W\n",
    "\n",
    "\n",
    "class SegmentSumFn(GraphConvFn):\n",
    "\n",
    "    def __init__(self, idx, num_seg):\n",
    "        # build A\n",
    "        # construct coo\n",
    "        data = nd.ones(idx.shape[0], ctx=idx.context, dtype='int64')\n",
    "        row, col = idx, nd.arange(idx.shape[0], ctx=idx.context, dtype='int64')\n",
    "        shape = (num_seg, int(idx.shape[0]))\n",
    "        sparse = nd.sparse.csr_matrix((data, (row, col)), shape=shape,\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        super(SegmentSumFn, self).__init__(sparse)\n",
    "\n",
    "        sparse = nd.sparse.csr_matrix((data, (col, row)), shape=(shape[1], shape[0]),\n",
    "                                      ctx=idx.context, dtype='float32')\n",
    "        self.A_T = sparse\n",
    "\n",
    "\n",
    "def squeeze(input, axis):\n",
    "    assert input.shape[axis] == 1\n",
    "\n",
    "    new_shape = list(input.shape)\n",
    "    del new_shape[axis]\n",
    "\n",
    "    return input.reshape(new_shape)\n",
    "\n",
    "\n",
    "def unsqueeze(input, axis):\n",
    "    return nd.expand_dims(input, axis=axis)\n",
    "\n",
    "\n",
    "def logsumexp(inputs, axis=None, keepdims=False):\n",
    "    \"\"\"Numerically stable logsumexp.\n",
    "    Args:\n",
    "        inputs: A Variable with any shape.\n",
    "        axis: An integer.\n",
    "        keepdims: A boolean.\n",
    "    Returns:\n",
    "        Equivalent of log(sum(exp(inputs), dim=dim, keepdim=keepdim)).\n",
    "    Adopted from: https://github.com/pytorch/pytorch/issues/2591\n",
    "    \"\"\"\n",
    "    # For a 1-D array x (any array along a single dimension),\n",
    "    # log sum exp(x) = s + log sum exp(x - s)\n",
    "    # with s = max(x) being a common choice.\n",
    "    if axis is None:\n",
    "        inputs = inputs.reshape([-1])\n",
    "        axis = 0\n",
    "    s = nd.max(inputs, axis=axis, keepdims=True)\n",
    "    outputs = s + (inputs - s).exp().sum(axis=axis, keepdims=True).log()\n",
    "    if not keepdims:\n",
    "        outputs = nd.sum(outputs, axis=axis, keepdims=False)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    activation_dict = {\n",
    "        'relu':nd.relu,\n",
    "        'tanh':nd.tanh\n",
    "    }\n",
    "    return activation_dict[name]\n",
    "\n",
    "\"\"\"# Defining neural networks\"\"\"\n",
    "\n",
    "class Linear_BN(nn.Sequential):\n",
    "    def __init__(self, F_in, F_out):\n",
    "        super(Linear_BN, self).__init__()\n",
    "        self.add(nn.Dense(F_out, in_units=F_in, use_bias=False))\n",
    "        self.add(BatchNorm(in_channels=F_out))\n",
    "\n",
    "\n",
    "class GraphConv(nn.Block):\n",
    "\n",
    "    def __init__(self, Fin, Fout, D):\n",
    "        super(GraphConv, self).__init__()\n",
    "\n",
    "        # model settings\n",
    "        self.Fin = Fin\n",
    "        self.Fout = Fout\n",
    "        self.D = D\n",
    "\n",
    "        # model parameters\n",
    "        self.W = self.params.get('w', shape=(self.Fin * (self.D + 1), self.Fout),\n",
    "                                 init=None, allow_deferred_init=False)\n",
    "\n",
    "    def forward(self, X, A_list):\n",
    "        try:\n",
    "            assert len(A_list) == self.D\n",
    "        except AssertionError as e:\n",
    "            print(self.D, len(A_list))\n",
    "            raise e\n",
    "        return EfficientGraphConvFn(A_list)(X, self.W.data(X.context))\n",
    "\n",
    "\n",
    "class Policy(nn.Block):\n",
    "\n",
    "    def __init__(self, F_in, F_h, N_A, N_B, k=1):\n",
    "        super(Policy, self).__init__()\n",
    "        self.F_in = F_in # number of input features for each atom\n",
    "        self.F_h = F_h # number of context variables\n",
    "        self.N_A = N_A # number of atom types\n",
    "        self.N_B = N_B # number of bond types\n",
    "        self.k = k # number of softmax used in the mixture\n",
    "\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.linear_h = Linear_BN(F_in * 2, self.F_h * k)\n",
    "            self.linear_h_t = Linear_BN(F_in, self.F_h * k)\n",
    "\n",
    "            self.linear_x = nn.Dense(self.N_B + self.N_B*self.N_A, in_units=self.F_h)\n",
    "            self.linear_x_t = nn.Dense(1, in_units=self.F_h)\n",
    "\n",
    "            if self.k > 1:\n",
    "                self.linear_pi = nn.Dense(self.k, in_units=self.F_in)\n",
    "            else:\n",
    "                self.linear_pi = None\n",
    "\n",
    "    def forward(self, X, NX, NX_rep, X_end=None):\n",
    "        # segment mean for X\n",
    "        if X_end is None:\n",
    "            X_end = SegmentSumFn(NX_rep, NX.shape[0])(X)/nd.cast(fn.unsqueeze(NX, 1), 'float32')\n",
    "        X = nd.concat(X, X_end[NX_rep, :], dim=1)\n",
    "\n",
    "        X_h = nd.relu(self.linear_h(X)).reshape([-1, self.F_h])\n",
    "        X_h_end = nd.relu(self.linear_h_t(X_end)).reshape([-1, self.F_h])\n",
    "\n",
    "        X_x = nd.exp(self.linear_x(X_h)).reshape([-1, self.k, self.N_B + self.N_B*self.N_A])\n",
    "        X_x_end = nd.exp(self.linear_x_t(X_h_end)).reshape([-1, self.k, 1])\n",
    "\n",
    "        X_sum = nd.sum(SegmentSumFn(NX_rep, NX.shape[0])(X_x), -1, keepdims=True) + X_x_end\n",
    "        X_sum_gathered = X_sum[NX_rep, :, :]\n",
    "\n",
    "        X_softmax = X_x / X_sum_gathered\n",
    "        X_softmax_end = X_x_end/ X_sum\n",
    "\n",
    "        if self.k > 1:\n",
    "            pi = unsqueeze(nd.softmax(self.linear_pi(X_end), axis=1), -1)\n",
    "            pi_gathered = pi[NX_rep, :, :]\n",
    "\n",
    "            X_softmax = nd.sum(X_softmax * pi_gathered, axis=1)\n",
    "            X_softmax_end = nd.sum(X_softmax_end * pi, axis=1)\n",
    "        else:\n",
    "            X_softmax = squeeze(X_softmax, 1)\n",
    "            X_softmax_end = squeeze(X_softmax_end, 1)\n",
    "\n",
    "        # generate output probabilities\n",
    "        connect, append = X_softmax[:, :self.N_B], X_softmax[:, self.N_B:]\n",
    "        append = append.reshape([-1, self.N_A, self.N_B])\n",
    "        end = squeeze(X_softmax_end, -1)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Block):\n",
    "\n",
    "    def __init__(self, in_channels, momentum=0.9, eps=1e-5):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.F = in_channels\n",
    "\n",
    "        self.bn_weight = self.params.get('bn_weight', shape=(self.F,), init=mx.init.One(),\n",
    "                                         allow_deferred_init=False)\n",
    "        self.bn_bias = self.params.get('bn_bias', shape=(self.F,), init=mx.init.Zero(),\n",
    "                                       allow_deferred_init=False)\n",
    "\n",
    "        self.running_mean = self.params.get('running_mean', grad_req='null',\n",
    "                                            shape=(self.F,),\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False,\n",
    "                                            differentiable=False)\n",
    "        self.running_var = self.params.get('running_var', grad_req='null',\n",
    "                                           shape=(self.F,),\n",
    "                                           init=mx.init.One(),\n",
    "                                           allow_deferred_init=False,\n",
    "                                           differentiable=False)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        if autograd.is_training():\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=False)\n",
    "        else:\n",
    "            return nd.BatchNorm(x,\n",
    "                                gamma=self.bn_weight.data(x.context),\n",
    "                                beta=self.bn_bias.data(x.context),\n",
    "                                moving_mean=self.running_mean.data(x.context),\n",
    "                                moving_var=self.running_var.data(x.context),\n",
    "                                eps=self.eps, momentum=self.momentum,\n",
    "                                use_global_stats=True)\n",
    "\n",
    "\"\"\"# Building generative network\"\"\"\n",
    "\n",
    "class MoleculeGenerator(nn.Block):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 *args, **kwargs):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.N_A = N_A\n",
    "        self.N_B = N_B\n",
    "        self.D = D\n",
    "        self.F_e = F_e\n",
    "        self.F_skip = F_skip\n",
    "        self.F_c = list(F_c) if isinstance(F_c, tuple) else F_c\n",
    "        self.Fh_policy = Fh_policy\n",
    "        self.activation = get_activation(activation)\n",
    "\n",
    "        with self.name_scope():\n",
    "            # embeddings\n",
    "            self.embedding_atom = nn.Embedding(self.N_A, self.F_e)\n",
    "            self.embedding_mask = nn.Embedding(3, self.F_e)\n",
    "\n",
    "            # graph conv\n",
    "            self._build_graph_conv(*args, **kwargs)\n",
    "\n",
    "            # fully connected\n",
    "            self.dense = nn.Sequential()\n",
    "            for i, (f_in, f_out) in enumerate(zip([self.F_skip, ] + self.F_c[:-1], self.F_c)):\n",
    "                self.dense.add(Linear_BN(f_in, f_out))\n",
    "\n",
    "            # policy\n",
    "            self.policy_0 = self.params.get('policy_0', shape=[self.N_A, ],\n",
    "                                            init=mx.init.Zero(),\n",
    "                                            allow_deferred_init=False)\n",
    "            self.policy_h = Policy(self.F_c[-1], self.Fh_policy, self.N_A, self.N_B)\n",
    "\n",
    "        self.mode = 'loss'\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_graph_conv(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _policy_0(self, ctx):\n",
    "        policy_0 = nd.exp(self.policy_0.data(ctx))\n",
    "        policy_0 = policy_0/policy_0.sum()\n",
    "        return policy_0\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _likelihood(self, init, append, connect, end,\n",
    "                    action_0, actions, iw_ids, log_p_sigma,\n",
    "                    batch_size, iw_size, tox_class_batch):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # decompose action:\n",
    "        action_type, node_type, edge_type, append_pos, connect_pos = \\\n",
    "            actions[:, 0], actions[:, 1], actions[:, 2], actions[:, 3], actions[:, 4]\n",
    "        _log_mask = lambda _x, _mask: _mask * nd.log(_x + 1e-10) + (1- _mask) * nd.zeros_like(_x)\n",
    "       \n",
    "        init = init.reshape([batch_size * iw_size, self.N_A])\n",
    "       \n",
    "        index = nd.stack(nd.arange(action_0.shape[0], ctx=action_0.context, dtype='int32'), action_0, axis=0)\n",
    "        # using action type, we are only taking those probabilities which corresponds to init prob\n",
    "        loss_init = nd.log(nd.gather_nd(init, index) + 1e-10)\n",
    "\n",
    "        # end\n",
    "    \n",
    "        loss_end = _log_mask(end, nd.cast(action_type == 2, 'float32'))\n",
    "\n",
    "\n",
    "        # append\n",
    "        index = nd.stack(append_pos, node_type, edge_type, axis=0)\n",
    "        loss_append = _log_mask(nd.gather_nd(append, index), nd.cast(action_type == 0, 'float32'))\n",
    "\n",
    "        # connect\n",
    "        index = nd.stack(connect_pos, edge_type, axis=0)\n",
    "        loss_connect = _log_mask(nd.gather_nd(connect, index), nd.cast(action_type == 1, 'float32'))\n",
    "        # using action type, we are only taking those probabilities which corresponds to connect prob\n",
    "\n",
    "        ## Please note that all the probabilities as of now are log prob as we have applied log_mask to absolute scores\n",
    "\n",
    "\n",
    "        # sum up results\n",
    "        log_p_x = loss_end + loss_append + loss_connect\n",
    "        log_p_x = squeeze(SegmentSumFn(iw_ids, batch_size*iw_size)(unsqueeze(log_p_x, -1)), -1)\n",
    "        \n",
    "        log_p_x = log_p_x + loss_init\n",
    "\n",
    "        # reshape\n",
    "        log_p_x = log_p_x.reshape([batch_size, iw_size])\n",
    "        log_p_sigma = log_p_sigma.reshape([batch_size, iw_size])\n",
    "        l = log_p_x - log_p_sigma\n",
    "        l = logsumexp(l, axis=1) - math.log(float(iw_size))\n",
    "\n",
    "        \n",
    "        # Aplly penalty based on toxicity label\n",
    "        for idx in range(len(l)):\n",
    "            class_tox = tox_class_batch[idx * self.k] # Since toxicity lable is repeated for all generation path for a molecule\n",
    "            if class_tox == 1:\n",
    "                l[idx] = -l[idx]\n",
    "\n",
    "        return l\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p,tox_class_batch, \\\n",
    "            batch_size, iw_size = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size, tox_class_batch)\n",
    "            if self.mode=='likelihood':\n",
    "                #print(f\"We are in likelihood mode\")\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            #print(f\"We are in decode_0 mode\")\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            #print(f\"We are in decode_step mode\")\n",
    "            X, A, NX, NX_rep, last_append_mask = input\n",
    "            return self._policy(X, A, NX, NX_rep, last_append_mask)\n",
    "\n",
    "\n",
    "class MoleculeGenerator_RNN(MoleculeGenerator):\n",
    "\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                 N_rnn, *args, **kwargs):\n",
    "        super(MoleculeGenerator_RNN, self).__init__(N_A, N_B, D, F_e, F_skip, F_c, Fh_policy, activation,\n",
    "                                                    *args, **kwargs)\n",
    "        self.N_rnn = N_rnn\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.rnn = gluon.rnn.GRU(hidden_size=self.F_c[-1],\n",
    "                                     num_layers=self.N_rnn,\n",
    "                                     layout='NTC', input_size=self.F_c[-1] * 2)\n",
    "\n",
    "    def _rnn_train(self, X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum-1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1)\n",
    "\n",
    "        # rnn\n",
    "        X = nd.take(X, indices=graph_to_rnn) # batch_size, iw_size, length, num_features\n",
    "        batch_size, iw_size, length, num_features = X.shape\n",
    "        X = X.reshape([batch_size*iw_size, length, num_features])\n",
    "        X = self.rnn(X)\n",
    "\n",
    "        X = X.reshape([batch_size, iw_size, length, -1])\n",
    "        X = nd.gather_nd(X, indices=rnn_to_graph)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _rnn_test(self, X, NX, NX_rep, NX_cum, h):\n",
    "        # note: one partition for one molecule\n",
    "        X_avg = SegmentSumFn(NX_rep, NX.shape[0])(X) / nd.cast(unsqueeze(NX, 1), 'float32')\n",
    "        X_curr = nd.take(X, indices=NX_cum - 1)\n",
    "        X = nd.concat(X_avg, X_curr, dim=1) # size: [NX, F_in * 2]\n",
    "\n",
    "        # rnn\n",
    "        X = unsqueeze(X, axis=1)\n",
    "        X, h = self.rnn(X, h)\n",
    "\n",
    "        X = squeeze(X, axis=1)\n",
    "        return X, h\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size, tox_class, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum = input\n",
    "\n",
    "            init = self._policy_0(X.context).tile([batch_size * iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "            l = self._likelihood(init, append, connect, end, action_0, actions, iw_ids, log_p, batch_size, iw_size, tox_class)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(input[0])\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "class _TwoLayerDense(nn.Block):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(_TwoLayerDense, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        with self.name_scope():\n",
    "            # config 1\n",
    "            self.input = nn.Dense(self.hidden_size, use_bias=False, in_units=self.input_size)\n",
    "            self.bn_input = BatchNorm(in_channels=self.hidden_size)\n",
    "            self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.hidden_size)\n",
    "\n",
    "            # config 2\n",
    "            #self.bn_input = BatchNorm(in_channels=self.input_size)\n",
    "            #self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.input_size)\n",
    "\n",
    "            # config 3\n",
    "            #self.input1 = nn.Dense(self.hidden_size, use_bias=False, in_units=self.input_size)\n",
    "            #self.bn_input1 = BatchNorm(in_channels=self.hidden_size)\n",
    "            #self.input2 = nn.Dense(self.hidden_size, use_bias=False, in_units=self.hidden_size)\n",
    "            #self.bn_input2 = BatchNorm(in_channels=self.hidden_size)\n",
    "            #self.output = nn.Dense(self.output_size, use_bias=True, in_units=self.hidden_size)\n",
    "\n",
    "    def forward(self, c):\n",
    "        # config 1\n",
    "        return nd.softmax(self.output(nd.relu(self.bn_input(self.input(c)))), axis=-1)\n",
    "\n",
    "        # config 2\n",
    "        #return nd.softmax(self.output(nd.relu(self.bn_input(c))), axis=-1)\n",
    "\n",
    "        # config 3\n",
    "        #return nd.softmax(self.output(nd.relu(self.bn_input2(self.input2(nd.relu(self.bn_input1(self.input1(c))))))), axis=-1)\n",
    "\n",
    "\n",
    "class CMoleculeGenerator_RNN(MoleculeGenerator_RNN):\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, N_A, N_B, N_C, D,\n",
    "                 F_e, F_skip, F_c, Fh_policy,\n",
    "                 activation, N_rnn,\n",
    "                 *args, **kwargs):\n",
    "        self.N_C = N_C # number of conditional variables\n",
    "        super(CMoleculeGenerator_RNN, self).__init__(N_A, N_B, D,\n",
    "                                                     F_e, F_skip, F_c, Fh_policy,\n",
    "                                                     activation, N_rnn,\n",
    "                                                     *args, **kwargs)\n",
    "        with self.name_scope():\n",
    "            self.dense_policy_0 = _TwoLayerDense(self.N_C, self.N_A * 3, self.N_A)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _graph_conv_forward(self, X, A, c, ids):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _policy_0(self, c):\n",
    "        return self.dense_policy_0(c) + 0.0 * self.policy_0.data(c.context)\n",
    "\n",
    "    def _policy(self, X, A, NX, NX_rep, last_append_mask,\n",
    "                graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "                c, ids):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A, c, ids)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol = self._rnn_train(X, NX, NX_rep, graph_to_rnn, rnn_to_graph, NX_cum)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end\n",
    "\n",
    "    def _decode_step(self, X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids):\n",
    "        # get initial embedding\n",
    "        X = self.embedding_atom(X) + self.embedding_mask(last_append_mask)\n",
    "\n",
    "        # convolution\n",
    "        X = self._graph_conv_forward(X, A, c, ids)\n",
    "\n",
    "        # linear\n",
    "        X = self.dense(X)\n",
    "\n",
    "        # rnn\n",
    "        X_mol, h = self._rnn_test(X, NX, NX_rep, NX_cum, h)\n",
    "\n",
    "        # policy\n",
    "        append, connect, end = self.policy_h(X, NX, NX_rep, X_mol)\n",
    "\n",
    "        return append, connect, end, h\n",
    "\n",
    "\n",
    "    def forward(self, *input):\n",
    "        if self.mode=='loss' or self.mode=='likelihood':\n",
    "            X, A, iw_ids, last_append_mask, \\\n",
    "            NX, NX_rep, action_0, actions, log_p, \\\n",
    "            batch_size, iw_size,tox_class, \\\n",
    "            graph_to_rnn, rnn_to_graph, NX_cum, \\\n",
    "            c, ids = input\n",
    "\n",
    "            init = nd.tile(unsqueeze(self._policy_0(c), axis=1), [1, iw_size, 1])\n",
    "            append, connect, end = self._policy(X, A, NX, NX_rep, last_append_mask,\n",
    "                                                graph_to_rnn, rnn_to_graph, NX_cum,\n",
    "                                                c, ids)\n",
    "            l = self._likelihood(init, append, connect, end,\n",
    "                                 action_0, actions, iw_ids, log_p,\n",
    "                                 batch_size, iw_size, tox_class)\n",
    "            if self.mode=='likelihood':\n",
    "                return l\n",
    "            else:\n",
    "                return -l.mean()\n",
    "        elif self.mode == 'decode_0':\n",
    "            return self._policy_0(*input)\n",
    "        elif self.mode == 'decode_step':\n",
    "            X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids = input\n",
    "            return self._decode_step(X, A, NX, NX_rep, last_append_mask, NX_cum, h, c, ids)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "class CVanillaMolGen_RNN(CMoleculeGenerator_RNN):\n",
    "\n",
    "    def __init__(self, N_A, N_B, N_C, D,\n",
    "                 F_e, F_h, F_skip, F_c, Fh_policy,\n",
    "                 activation, N_rnn, rename=False):\n",
    "        self.rename = rename\n",
    "        super(CVanillaMolGen_RNN, self).__init__(N_A, N_B, N_C, D,\n",
    "                                                 F_e, F_skip, F_c, Fh_policy,\n",
    "                                                 activation, N_rnn,\n",
    "                                                 F_h)\n",
    "        with self.name_scope():\n",
    "            self.dense_policy_0 = _TwoLayerDense(N_C, N_A * 3, N_A)\n",
    "            self.dense_policy_0.initialize(mx.init.Xavier(), ctx=mx.gpu())  # Initialize the parameters\n",
    "\n",
    "    def _build_graph_conv(self, F_h):\n",
    "        self.F_h = list(F_h) if isinstance(F_h, tuple) else F_h\n",
    "        self.conv, self.bn = [], []\n",
    "        for i, (f_in, f_out) in enumerate(zip([self.F_e] + self.F_h[:-1], self.F_h)):\n",
    "            conv = GraphConv(f_in, f_out, self.N_B + self.D)\n",
    "            self.conv.append(conv)\n",
    "            self.register_child(conv)\n",
    "\n",
    "            if i != 0:\n",
    "                bn = BatchNorm(in_channels=f_in)\n",
    "                self.register_child(bn)\n",
    "            else:\n",
    "                bn = None\n",
    "            self.bn.append(bn)\n",
    "\n",
    "        self.bn_skip = BatchNorm(in_channels=sum(self.F_h))\n",
    "        self.linear_skip = Linear_BN(sum(self.F_h), self.F_skip)\n",
    "\n",
    "        # projectors for conditional variable (protein embedding)\n",
    "        self.linear_c = []\n",
    "        for i, f_out in enumerate(self.F_h):\n",
    "            if self.rename:\n",
    "                linear_c = nn.Dense(f_out, use_bias=False, in_units=self.N_C, prefix='cond_{}'.format(i))\n",
    "            else:\n",
    "                linear_c = nn.Dense(f_out, use_bias=False, in_units=self.N_C)\n",
    "            self.register_child(linear_c)\n",
    "            self.linear_c.append(linear_c)\n",
    "\n",
    "    def _graph_conv_forward(self, X, A, c, ids):\n",
    "        X_out = [X]\n",
    "        for conv, bn, linear_c in zip(self.conv, self.bn, self.linear_c):\n",
    "            X = X_out[-1]\n",
    "            if bn is not None:\n",
    "                X_out.append(conv(self.activation(bn(X)), A) + linear_c(c)[ids, :])\n",
    "            else:\n",
    "                X_out.append(conv(X, A) + linear_c(c)[ids, :])\n",
    "        X_out = nd.concat(*X_out[1:], dim=1)\n",
    "        return self.activation(self.linear_skip(self.activation(self.bn_skip(X_out))))\n",
    "\n",
    "# Early Stopping Implementation\n",
    "class EarlyStopping(object):\n",
    "\n",
    "    def __init__(self, patience=5, delta=1e-2, less_is_better=True):\n",
    "\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.less_is_better = less_is_better\n",
    "        self.best = math.inf if less_is_better else -math.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def update(self, loss):\n",
    "        if self.best is None:\n",
    "            self.best = loss\n",
    "            return\n",
    "\n",
    "        if self.less_is_better:\n",
    "            # Best loss updated.\n",
    "            if loss < self.best - self.delta:\n",
    "                self.best = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:\n",
    "            # Best loss updated.\n",
    "            if loss > self.best + self.delta:\n",
    "                self.best = loss\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "    def step(self, loss):\n",
    "        self.update(loss)\n",
    "\n",
    "        # Return True if the counter exceeded patience.\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.best\n",
    "\n",
    "\"\"\"# Loading and preprocessing the dataset\"\"\"\n",
    "\n",
    "num_workers=0\n",
    "if all([os.path.isfile(os.path.join(ckpt_dir, _n)) for _n in ['log.out', 'ckpt.params', 'trainer.status']]):\n",
    "    is_continuous = True\n",
    "else:\n",
    "    is_continuous = False\n",
    "\n",
    "cond = Delimited()\n",
    "\n",
    "if train_only:\n",
    "    dataset = Lambda(dataset_train, lambda _x: _x.strip('\\n').strip('\\r'))\n",
    "\n",
    "    # get sampler and loader for training set\n",
    "    sampler_train = BalancedSampler(cost=[len(l.split('\\t')[0]) for l in dataset], batch_size=batch_size)\n",
    "    loader_train = CMolRNNLoader(dataset, batch_sampler=sampler_train, num_workers=num_workers,\n",
    "                                      k=k, p=p, conditional=cond)\n",
    "\n",
    "    loader_test = []\n",
    "\n",
    "else:\n",
    "    if all([os.path.isfile(os.path.join(ckpt_dir, _n)) for _n in ['log.out', 'ckpt.params', 'trainer.status']]):\n",
    "        is_continuous = True\n",
    "    else:\n",
    "        is_continuous = False\n",
    "\n",
    "    db_train = Lambda(dataset_train, lambda _x: _x.strip('\\n').strip('\\r'))\n",
    "    # get sampler and loader for training set\n",
    "    sampler_train = BalancedSampler(cost=[len(l.split('\\t')[0]) for l in db_train], batch_size=batch_size)\n",
    "    loader_train = CMolRNNLoader(db_train, batch_sampler=sampler_train, num_workers=num_workers,\n",
    "                                      k=k, p=p, conditional=cond)\n",
    "\n",
    "    db_test = Lambda(dataset_test, lambda _x: _x.strip('\\n').strip('\\r'))\n",
    "    # get sampler and loader for test set\n",
    "    sampler_test = BalancedSampler(cost=[len(l.split('\\t')[0]) for l in db_test], batch_size=batch_size_test)\n",
    "    loader_test = CMolRNNLoader(db_test, batch_sampler=sampler_test, num_workers=num_workers,\n",
    "                                      k=k, p=p, conditional=cond)\n",
    "\n",
    "# get iterator\n",
    "it_train, it_test = iter(loader_train), iter(loader_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"is continuous is {is_continuous}\")\n",
    "# build model\n",
    "if not is_continuous:\n",
    "    configs = {'N_C': N_C,\n",
    "               'F_e': F_e,\n",
    "               'F_h': F_h,\n",
    "               'F_skip': F_skip,\n",
    "               'F_c': F_c,\n",
    "               'Fh_policy': Fh_policy,\n",
    "               'activation': activation,\n",
    "               'rename': False,\n",
    "               'N_rnn': N_rnn}\n",
    "    with open(os.path.join(ckpt_dir, 'configs.json'), 'w') as f:\n",
    "        json.dump(configs, f)\n",
    "else:\n",
    "    with open(os.path.join(ckpt_dir, 'configs.json')) as f:\n",
    "        configs = json.load(f)\n",
    "\n",
    "model = CVanillaMolGen_RNN(get_mol_spec().num_atom_types, get_mol_spec().num_bond_types, D=2, **configs)\n",
    "\n",
    "\n",
    "\n",
    "#model.collect_params().initialize(mx.init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "\n",
    "#model.load_parameters(os.path.join(ckpt_dir, 'ckpt.params'), ctx=ctx, allow_missing=True)\n",
    "#model.collect_params().initialize(mx.init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "\n",
    "ctx = mx.gpu()\n",
    "if not is_continuous:\n",
    "    print(\"Parameters are initialized...\")\n",
    "    model.collect_params().initialize(mx.init.Xavier(), force_reinit=True, ctx=ctx)\n",
    "else:\n",
    "\n",
    "    model.load_parameters(os.path.join(ckpt_dir, 'ckpt.params'), ctx=ctx)\n",
    "    print(\"Parameters are loaded...\")\n",
    "\n",
    "\n",
    "# construct optimizer\n",
    "opt = mx.optimizer.Adam(learning_rate=lr, clip_gradient=clip_grad)\n",
    "trainer = gluon.Trainer(model.collect_params(), opt)\n",
    "if is_continuous:\n",
    "    trainer.load_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for layer_name, layer_params in model.collect_params().items():\n",
    "    print(layer_name)\n",
    "    print(layer_params.shape)\n",
    "    print(model.collect_params()[layer_name].grad_req)\n",
    "\n",
    "\n",
    "if not is_continuous:\n",
    "    t0 = time.time()\n",
    "    global_counter = 0\n",
    "    print(f\"Training is starting from scratch...\")\n",
    "else:\n",
    "    with open(os.path.join(ckpt_dir, 'log.out')) as f:\n",
    "        records = f.readlines()\n",
    "        if records[-1] != 'Training finished\\n':\n",
    "            final_record = records[-1]\n",
    "        else:\n",
    "            final_record = records[-2]\n",
    "    count, t_final = int(final_record.split('\\t')[0]), float(final_record.split('\\t')[1])\n",
    "    t0 = time.time() - t_final * 60\n",
    "    print(\"Global count is: \", count)\n",
    "    global_counter = count\n",
    "\n",
    "eval_now=True\n",
    "epoch_no = 1\n",
    "break_train_loop = False\n",
    "early_stopping = EarlyStopping(patience=patience, delta=1e-2, less_is_better=True)\n",
    "\n",
    "with open(os.path.join(ckpt_dir, 'log.out'),\n",
    "          mode='w' if not is_continuous else 'a') as f:\n",
    "    if not is_continuous:\n",
    "        f.write('Training started...\\n')\n",
    "        f.write('step\\ttime(m)\\tloss\\tlr\\n')\n",
    "    while True:\n",
    "        global_counter += 1\n",
    "        #print(f\"global counter is {global_counter}\")\n",
    "        eval_now = False\n",
    "        try:\n",
    "            inputs = next(it_train)\n",
    "        except StopIteration:\n",
    "            eval_now = True\n",
    "            print('Epoch {} complete.'.format(epoch_no))\n",
    "            f.write('\\nEpoch {} complete.\\n'.format(epoch_no))\n",
    "            epoch_no += 1\n",
    "            it_train = iter(loader_train)\n",
    "            inputs = next(it_train)\n",
    "\n",
    "        # move to gpu\n",
    "        inputs = CMolRNNLoader.from_numpy_to_tensor(inputs)\n",
    "\n",
    "        with autograd.record():\n",
    "            loss = [(model(*inputs)).as_in_context(mx.gpu())]\n",
    "            loss = sum(loss)\n",
    "            loss.backward()\n",
    "\n",
    "        nd.waitall()\n",
    "        gc.collect()\n",
    "\n",
    "        trainer.step(batch_size=1, ignore_stale_grad=True)\n",
    "\n",
    "        if global_counter % decay_step == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * (1.0 - decay))\n",
    "\n",
    "        if global_counter % summary_step == 0:\n",
    "            model.save_parameters(os.path.join(ckpt_dir, 'ckpt.params'))\n",
    "            trainer.save_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "            print('Training loss = ', (sum(loss)).asnumpy().item())\n",
    "            f.write('{}\\t{:.2f}\\t{:.4f}\\t{:.8f}\\n'.format(global_counter, float(time.time() - t0) / 60, (sum(loss)).asnumpy().item(),\n",
    "                                              trainer.learning_rate))\n",
    "            f.flush()\n",
    "\n",
    "            print('{} iterations done!'.format(global_counter))\n",
    "\n",
    "        if eval_now:\n",
    "            if train_only:\n",
    "                loss = (sum(loss)).asnumpy().item()\n",
    "            else:\n",
    "                del loss, inputs\n",
    "                gc.collect()\n",
    "\n",
    "                loss = []\n",
    "                while True:\n",
    "                    try:\n",
    "                        inputs = next(it_test)\n",
    "                        with autograd.predict_mode():\n",
    "                            # move to gpu\n",
    "                            inputs = CMolRNNLoader.from_numpy_to_tensor(inputs)\n",
    "                            loss.append(sum([(model(*inputs)).as_in_context(mx.gpu())]))\n",
    "                    except StopIteration:\n",
    "                        loss = (sum(loss)/len(loss)).asnumpy().item()\n",
    "                        print('Validation loss = ', loss)\n",
    "                        f.write('\\nValidation loss: {}\\n\\n'.format(loss))\n",
    "                        stop = early_stopping.step(loss)\n",
    "                        if stop:\n",
    "                            best_score = early_stopping.get_best_score()\n",
    "                            print(f'Early stopping! Best validation loss: {best_score}')\n",
    "                            f.write(f'\\n\\nEarly stopping! Best validation loss: {best_score}\\n\\n')\n",
    "                            break_train_loop = True\n",
    "                        if not break_train_loop:\n",
    "                            f.write('\\nstep\\ttime(h)\\tloss\\tlr\\n')\n",
    "                        it_test = iter(loader_test)\n",
    "                        inputs = next(it_test)\n",
    "                        break\n",
    "\n",
    "        if break_train_loop:\n",
    "            break\n",
    "\n",
    "        if train_only and epoch_no > max_epochs:\n",
    "            break\n",
    "\n",
    "    # save before exit\n",
    "    model.save_parameters(os.path.join(ckpt_dir, 'ckpt.params'))\n",
    "    trainer.save_states(os.path.join(ckpt_dir, 'trainer.status'))\n",
    "\n",
    "    f.write('Training finished\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
